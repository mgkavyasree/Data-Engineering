{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_operations.py\n",
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('superstore.csv')\n",
    "\n",
    "# view structure\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "\n",
    "# select specific columns\n",
    "print(df[['customer', 'product', 'profit']])\n",
    "\n",
    "# filter where profit > 2000 and discount = 0\n",
    "filtered = df[(df['profit'] > 2000) & (df['discount'] == 0)]\n",
    "print(filtered)\n",
    "\n",
    "# sort by profit descending\n",
    "sorted_df = df.sort_values(by='profit', ascending=False)\n",
    "print(sorted_df)\n",
    "\n",
    "# group by category for total profit and average discount\n",
    "grouped = df.groupby('category').agg({'profit': 'sum', 'discount': 'mean'})\n",
    "print(grouped)\n",
    "\n",
    "# add totalprice column\n",
    "df['totalprice'] = df['quantity'] * df['unitprice']\n",
    "\n",
    "# drop subcategory column\n",
    "df.drop(columns=['subcategory'], inplace=True)\n",
    "\n",
    "# fill missing discount values\n",
    "df['discount'] = df['discount'].fillna(0.10)\n",
    "\n",
    "# apply classification based on profit\n",
    "def classify(row):\n",
    "    if row['profit'] > 4000:\n",
    "        return 'high'\n",
    "    elif row['profit'] > 0:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "df['profit_class'] = df.apply(classify, axis=1)\n",
    "\n",
    "# final view\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark_operations.py \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, avg, year, month\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# initialize spark session\n",
    "spark = SparkSession.builder.appName(\"superstore\").getOrCreate()\n",
    "\n",
    "# load data\n",
    "df_spark = spark.read.csv(\"superstore.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# show schema and sample\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)\n",
    "\n",
    "# select and rename columns\n",
    "df_spark.select(col(\"customer\").alias(\"client\"), \"product\", \"profit\").show()\n",
    "\n",
    "# filter by segment and profit\n",
    "df_spark.filter((col(\"segment\") == \"Consumer\") & (col(\"profit\") < 1000)).show()\n",
    "\n",
    "# region-wise average profit\n",
    "df_spark.groupBy(\"region\").agg(avg(\"profit\").alias(\"avg_profit\")).show()\n",
    "\n",
    "# add totalprice column\n",
    "df_spark = df_spark.withColumn(\"totalprice\", col(\"quantity\") * col(\"unitprice\"))\n",
    "\n",
    "# classify orders by profit\n",
    "df_spark = df_spark.withColumn(\"profit_class\",\n",
    "    when(col(\"profit\") > 2000, \"high\")\n",
    "    .when(col(\"profit\") <= 0, \"loss\")\n",
    "    .otherwise(\"medium\")\n",
    ")\n",
    "\n",
    "# drop subcategory column\n",
    "df_spark = df_spark.drop(\"subcategory\")\n",
    "\n",
    "# fill null discount values\n",
    "df_spark = df_spark.fillna({\"discount\": 0.10})\n",
    "\n",
    "# convert orderdate to date and extract year, month\n",
    "df_spark = df_spark.withColumn(\"orderdate\", col(\"orderdate\").cast(DateType()))\n",
    "df_spark = df_spark.withColumn(\"order_year\", year(\"orderdate\"))\n",
    "df_spark = df_spark.withColumn(\"order_month\", month(\"orderdate\"))\n",
    "\n",
    "# final result\n",
    "df_spark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask_operations.py \n",
    "# install dask \n",
    "!pip install dask\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# load dataset\n",
    "ddf = dd.read_csv('superstore.csv')\n",
    "\n",
    "# compute average discount by category\n",
    "avg_discount = ddf.groupby('category')['discount'].mean().compute()\n",
    "print(avg_discount)\n",
    "\n",
    "# filter by quantity > 1 and high profit\n",
    "filtered_ddf = ddf[(ddf['quantity'] > 1) & (ddf['profit'] > 2000)]\n",
    "\n",
    "# save filtered result\n",
    "filtered_ddf.compute().to_csv(\"filtered_superstore.csv\", index=False)\n",
    "print(\"filtered data saved to filtered_superstore.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_reader.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# initialize spark session\n",
    "spark = SparkSession.builder.appName(\"nested_json\").getOrCreate()\n",
    "\n",
    "# read nested json\n",
    "df_json = spark.read.json(\"orders.json\", multiLine=True)\n",
    "\n",
    "# show schema and selected fields\n",
    "df_json.printSchema()\n",
    "df_json.select(\"orderid\", \"customer.name\", \"details.profit\").show()\n"
   ]
  }
 ],
 "metadata": {
  "language": "python",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
