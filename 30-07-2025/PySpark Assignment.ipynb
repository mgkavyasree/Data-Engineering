# part 1: environment setup
!apt-get install openjdk-11-jdk -y
!wget -q https://downloads.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
!tar xf spark-3.3.0-bin-hadoop3.tgz
!pip install -q findspark

import os
os.environ["java_home"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["spark_home"] = "/content/spark-3.3.0-bin-hadoop3"

import findspark
findspark.init()

from pyspark.sql import sparkSession
from pyspark.sql.functions import col, sum, desc, avg, when

spark = sparkSession.builder.appName("productsalesanalysis").getOrCreate()

# part 2: load sales data from csv
csv_data = """orderid,product,category,quantity,unitprice,region
1001,mobile,electronics,2,15000,north
1002,laptop,electronics,1,55000,south
1003,t-shirt,apparel,3,500,east
1004,jeans,apparel,2,1200,north
1005,tv,electronics,1,40000,west
1006,shoes,footwear,4,2000,south
1007,watch,accessories,2,3000,east
1008,headphones,electronics,3,2500,north
"""

with open("sales.csv", "w") as file:
    file.write(csv_data)

df = spark.read.csv("sales.csv", header=True, inferSchema=True)
print("schema:")
df.printSchema()
print("top 5 rows:")
df.show(5)

# part 3: business questions

# 1. add totalprice column
df = df.withColumn("totalprice", col("quantity") * col("unitprice"))

# 2. total revenue
total_revenue = df.agg(sum("totalprice").alias("total_revenue"))
print("total revenue:")
total_revenue.show()

# 3. category-wise revenue in descending order
category_revenue = df.groupBy("category").agg(sum("totalprice").alias("revenue"))
print("category-wise revenue:")
category_revenue.orderBy(desc("revenue")).show()

# 4. region with highest number of orders
region_orders = df.groupBy("region").count()
print("region with most orders:")
region_orders.orderBy(desc("count")).show(1)

# 5. average unit price per category
print("average unit price per category:")
df.groupBy("category").agg(avg("unitprice").alias("average_price")).show()

# 6. orders with totalprice > 30000
print("orders with total price > 30000:")
df.filter(col("totalprice") > 30000).show()

# part 4: data transformations

# 1. create highvalueorder column
df = df.withColumn("highvalueorder", when(col("totalprice") > 20000, "yes").otherwise("no"))

# 2. filter high value orders in north region
print("high value orders in north region:")
df.filter((col("highvalueorder") == "yes") & (col("region") == "north")).show()

# 3. count high value orders per region
print("count of high value orders by region:")
df.filter(col("highvalueorder") == "yes").groupBy("region").count().show()

# part 5: save results

# save high value orders as csv
df.filter(col("highvalueorder") == "yes") \
  .write.mode("overwrite") \
  .option("header", True) \
  .csv("high_value_orders.csv")

print("file saved as high_value_orders.csv")
