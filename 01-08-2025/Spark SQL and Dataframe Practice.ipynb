{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Setup and Data Preparation\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import avg, count, col, when, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"employee_work_data_analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# simulate employee data\n",
    "data = [\n",
    "    Row(emp_id=101, name=\"Ravi\", department=\"engineering\", project=\"ai engine\", salary=95000, hours_per_week=42),\n",
    "    Row(emp_id=102, name=\"Sneha\", department=\"engineering\", project=\"data platform\", salary=87000, hours_per_week=45),\n",
    "    Row(emp_id=103, name=\"Kabir\", department=\"marketing\", project=\"product launch\", salary=65000, hours_per_week=40),\n",
    "    Row(emp_id=104, name=\"Anita\", department=\"sales\", project=\"client outreach\", salary=70000, hours_per_week=38),\n",
    "    Row(emp_id=105, name=\"Divya\", department=\"engineering\", project=\"ai engine\", salary=99000, hours_per_week=48),\n",
    "    Row(emp_id=106, name=\"Amit\", department=\"marketing\", project=\"social media\", salary=62000, hours_per_week=35),\n",
    "    Row(emp_id=107, name=\"Priya\", department=\"hr\", project=\"policy revamp\", salary=58000, hours_per_week=37),\n",
    "    Row(emp_id=108, name=\"Manav\", department=\"sales\", project=\"lead gen\", salary=73000, hours_per_week=41),\n",
    "    Row(emp_id=109, name=\"Neha\", department=\"engineering\", project=\"security suite\", salary=91000, hours_per_week=46),\n",
    "    Row(emp_id=110, name=\"Farah\", department=\"hr\", project=\"onboarding\", salary=60000, hours_per_week=36)\n",
    "]\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Creating Views\n",
    "\n",
    "# create local temp view\n",
    "df.createOrReplaceTempView(\"employees_local\")\n",
    "\n",
    "# create global temp view\n",
    "df.createOrReplaceGlobalTempView(\"employees_global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Queries on Local View\n",
    "\n",
    "# 1. employees working on \"ai engine\"\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees_local \n",
    "    WHERE project = 'ai engine'\n",
    "\"\"\").show()\n",
    "\n",
    "# 2. marketing employees with salary > 60000\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees_local \n",
    "    WHERE department = 'marketing' AND salary > 60000\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. average salary by department\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) AS avg_salary \n",
    "    FROM employees_local \n",
    "    GROUP BY department\n",
    "\"\"\").show()\n",
    "\n",
    "# 4. top 3 highest paid employees\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees_local \n",
    "    ORDER BY salary DESC \n",
    "    LIMIT 3\n",
    "\"\"\").show()\n",
    "\n",
    "# 5. employees working > 40 hours\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees_local \n",
    "    WHERE hours_per_week > 40\n",
    "\"\"\").show()\n",
    "\n",
    "# 6. number of employees per project\n",
    "spark.sql(\"\"\"\n",
    "    SELECT project, COUNT(*) AS employee_count \n",
    "    FROM employees_local \n",
    "    GROUP BY project\n",
    "\"\"\").show()\n",
    "\n",
    "# 7. drop local view and try querying again\n",
    "spark.catalog.dropTempView(\"employees_local\")\n",
    "# spark.sql(\"SELECT * FROM employees_local\").show()  # will error if uncommented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Queries on Global View\n",
    "\n",
    "# 1. HR employees working < 38 hours\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM global_temp.employees_global \n",
    "    WHERE department = 'hr' AND hours_per_week < 38\n",
    "\"\"\").show()\n",
    "\n",
    "# 2. total salary by department\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, SUM(salary) AS total_salary \n",
    "    FROM global_temp.employees_global \n",
    "    GROUP BY department\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. add status column\n",
    "df_with_status = spark.sql(\"\"\"\n",
    "    SELECT *, \n",
    "        CASE \n",
    "            WHEN hours_per_week > 45 THEN 'overworked' \n",
    "            ELSE 'normal' \n",
    "        END AS status \n",
    "    FROM global_temp.employees_global\n",
    "\"\"\")\n",
    "df_with_status.show()\n",
    "\n",
    "# 4. number of employees per project\n",
    "spark.sql(\"\"\"\n",
    "    SELECT project, COUNT(*) AS total_employees \n",
    "    FROM global_temp.employees_global \n",
    "    GROUP BY project\n",
    "\"\"\").show()\n",
    "\n",
    "# 5. employees with salary above dept average\n",
    "df_global = spark.table(\"global_temp.employees_global\")\n",
    "dept_avg = df_global.groupBy(\"department\").agg(avg(\"salary\").alias(\"dept_avg_salary\"))\n",
    "above_avg = df_global.join(dept_avg, \"department\") \\\n",
    "    .filter(col(\"salary\") > col(\"dept_avg_salary\")) \\\n",
    "    .select(\"emp_id\", \"name\", \"department\", \"salary\")\n",
    "above_avg.show()\n",
    "\n",
    "# 6. open new session and query global view\n",
    "new_spark = SparkSession.builder \\\n",
    "    .appName(\"new_session\") \\\n",
    "    .getOrCreate()\n",
    "new_spark.sql(\"SELECT * FROM global_temp.employees_global\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Challenges\n",
    "\n",
    "# 1. rank employees by salary within each department\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df_ranked = df_global.withColumn(\"salary_rank\", rank().over(window_spec))\n",
    "df_ranked.select(\"emp_id\", \"name\", \"department\", \"salary\", \"salary_rank\").show()\n",
    "\n",
    "# 2. create view for engineering employees\n",
    "df_engineering = df_global.filter(col(\"department\") == \"engineering\")\n",
    "df_engineering.createOrReplaceTempView(\"engineering_employees\")\n",
    "spark.sql(\"SELECT * FROM engineering_employees\").show()\n",
    "\n",
    "# 3. create view for active employees (working >= 38 hours)\n",
    "df_active = df_global.filter(col(\"hours_per_week\") >= 38)\n",
    "df_active.createOrReplaceTempView(\"active_employees\")\n",
    "spark.sql(\"SELECT * FROM active_employees\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
